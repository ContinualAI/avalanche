{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "description: 'First things first: let''s start with a good model!'\n",
    "---\n",
    "\n",
    "# Models\n",
    "\n",
    "Welcome to the \"**Models**\" tutorial of the \"_From Zero to Hero_\" series. In this notebook we will talk about the features offered by the `models` _Avalanche_ sub-module.\n",
    "\n",
    "### Support for pytorch Modules\n",
    "\n",
    "Every continual learning experiment needs a model to train incrementally. You can use any `torch.nn.Module`, even pretrained models.  The `models` sub-module provides for you the most commonly used architectures in the CL literature.\n",
    "\n",
    "You can use any model provided in the [Pytorch](https://pytorch.org/) official ecosystem models as well as the ones provided by [pytorchcv](https://pypi.org/project/pytorchcv/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install avalanche-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from avalanche.models import SimpleCNN\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.models import SimpleMLP_TinyImageNet\n",
    "from avalanche.models import MobilenetV1\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model Expansion\n",
    "A continual learning model may change over time. As an example, a classifier may add new units for previously unseen classes, while progressive networks add a new set units after each experience. Avalanche provides `DynamicModule`s to support these use cases. `DynamicModule`s are `torch.nn.Module`s that provide an addition method, `adaptation`, that is used to update the model's architecture. The method takes a single argument, the data from the current experience.\n",
    "\n",
    "For example, an IncrementalClassifier updates the number of output units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      ")\n",
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      ")\n",
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=4, bias=True)\n",
      ")\n",
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=6, bias=True)\n",
      ")\n",
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=8, bias=True)\n",
      ")\n",
      "IncrementalClassifier(\n",
      "  (classifier): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.models import IncrementalClassifier\n",
    "\n",
    "benchmark = SplitMNIST(5, shuffle=False)\n",
    "model = IncrementalClassifier(in_features=784)\n",
    "\n",
    "print(model)\n",
    "for exp in benchmark.train_stream:\n",
    "    model.adaptation(exp.dataset)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, after each call to the `adaptation` method, the model adds 2 new units to account for the new classes. Notice that no learning occurs at this point since the method only modifies the model's architecture.\n",
    "\n",
    "Keep in mind that when you use Avalanche strategies you don't have to call the adaptation yourself. Avalanche strategies automatically call the model's adaptation and update the optimizer to include the new parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-Task models\n",
    "\n",
    "Some models, such as multi-head classifiers, are designed to exploit task labels. In Avalanche, such models are implemented as `MultiTaskModule`s. These are dynamic models (since they need to be updated whenever they encounter a new task) that have an additional `task_labels` argument in their `forward` method. `task_labels` is a tensor with a task id for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (2): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (2): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (3): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MultiHeadClassifier(\n",
      "  (classifiers): ModuleDict(\n",
      "    (0): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (2): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (3): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "    (4): IncrementalClassifier(\n",
      "      (classifier): Linear(in_features=784, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.models import MultiHeadClassifier\n",
    "\n",
    "benchmark = SplitMNIST(5, shuffle=False, return_task_id=True)\n",
    "model = MultiHeadClassifier(in_features=784)\n",
    "\n",
    "print(model)\n",
    "for exp in benchmark.train_stream:\n",
    "    model.adaptation(exp.dataset)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When you use a `MultiHeadClassifier`, a new head is initialized whenever a new task is encountered. Avalanche strategies automatically recognizes multi-task modules and provide the task labels to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How to define a multi-task Module\n",
    "If you want to define a custom multi-task module you need to override two methods: `adaptation` (if needed), and `forward_single_task`. The `forward` method of the base class will split the mini-batch by task-id and provide single task mini-batches to `forward_single_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.models import MultiTaskModule\n",
    "\n",
    "class CustomMTModule(MultiTaskModule):\n",
    "    def __init__(self, in_features, initial_out_features=2):\n",
    "        super().__init__()\n",
    "\n",
    "    def adaptation(self, dataset):\n",
    "        super().adaptation(dataset)\n",
    "        # your adaptation goes here\n",
    "\n",
    "    def forward_single_task(self, x, task_label):\n",
    "        # your forward goes here.\n",
    "        # task_label is a single integer\n",
    "        # the mini-batch is split by task-id inside the forward method.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alternatively, if you only want to convert a single-head model into a multi-head model, you can use the `as_multitask` wrapper, which converts the model for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): AdaptiveMaxPool2d(output_size=1)\n",
      "    (15): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "MultiTaskDecorator(\n",
      "  (model): SimpleCNN(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Dropout(p=0.25, inplace=False)\n",
      "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (11): Dropout(p=0.25, inplace=False)\n",
      "      (12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): AdaptiveMaxPool2d(output_size=1)\n",
      "      (15): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (classifier): Sequential()\n",
      "  )\n",
      "  (classifier): MultiHeadClassifier(\n",
      "    (classifiers): ModuleDict(\n",
      "      (0): IncrementalClassifier(\n",
      "        (classifier): Linear(in_features=64, out_features=10, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from avalanche.models import as_multitask\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "\n",
    "mt_model = as_multitask(model, 'classifier')\n",
    "print(mt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ü§ù Run it on Google Colab\n",
    "\n",
    "You can run _this chapter_ and play with it on Google Colaboratory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContinualAI/avalanche/blob/master/notebooks/from-zero-to-hero-tutorial/02_models.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
