{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "49eT_6SRDeLU"
      },
      "source": [
        "# Learn Avalanche in 5 Minutes\n",
        "## A Short Guide for Researchers on the Run"
      ],
      "id": "49eT_6SRDeLU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OF5XAUXeDeLW"
      },
      "source": [
        "*Avalanche* is mostly about making the life of a continual learning researcher easier.\n",
        "\n",
        "Below, you can see the main Avalanche modules and how they interact with each other."
      ],
      "id": "OF5XAUXeDeLW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hJnzE8zWDeLX"
      },
      "source": [
        "![avalanche](https://raw.githubusercontent.com/ContinualAI/avalanche/master/docs/gitbook/.gitbook/assets/avalanche.png)"
      ],
      "id": "hJnzE8zWDeLX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "klGUdVWzDeLX"
      },
      "source": [
        "**What are the three pillars of any respectful continual learning research project?**"
      ],
      "id": "klGUdVWzDeLX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tCvjneGODeLY"
      },
      "source": [
        "1. **`Benchmarks`**: Machine learning researchers need multiple benchmarks with efficient data handling utils to design and prototype new algorithms. Quantitative results on ever-changing benchmarks has been one of the driving forces of *Deep Learning*.\n",
        "1. **`Training`**: Efficient implementation and training of continual learning algorithms; comparisons with other baselines and state-of-the-art methods become fundamental to asses the quality of an original algorithmic proposal.\n",
        "1. **`Evaluation`**: *Training* utils and *Benchmarks* are not enough alone to push continual learning research forward. Comprehensive and sound *evaluation protocols* and *metrics* need to be employed as well.\n",
        "\n",
        "**With Avalanche, you can find all these three fundamental pieces together and much more, in a single and coherent, well-maintained codebase.**\n",
        "\n",
        "Let's take a quick tour on how you can use Avalanche for your research projects with a **5-minutes guide**, for *researchers on the run*!"
      ],
      "id": "tCvjneGODeLY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZC5bt6xnDeLY"
      },
      "source": [
        "Let's first **install Avalanche**. Please, check out our [How to Install](https://avalanche.continualai.org/getting-started/how-to-install) guide for further details."
      ],
      "id": "ZC5bt6xnDeLY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6LnmmMkJDeLZ"
      },
      "outputs": [],
      "source": [
        "!pip install avalanche-lib[all]\n",
        "!pip show avalanche-lib"
      ],
      "id": "6LnmmMkJDeLZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hoqS6mOEDeLa"
      },
      "source": [
        "## üèõÔ∏è General Architecture"
      ],
      "id": "hoqS6mOEDeLa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Mqc2TP3aDeLa"
      },
      "source": [
        "Avalanche is organized in five main modules:\n",
        "\n",
        "1. **Benchmarks**: This module maintains a uniform API for data handling: mostly generating a stream of data from one or more datasets. It contains all the major CL benchmarks (similar to what has been done for [torchvision](https://pytorch.org/docs/stable/torchvision/index.html)).\n",
        "1. **Training**: This module provides all the necessary utilities concerning model training. This includes simple and efficient ways of implement new continual learning strategies as well as a set pre-implemented CL baselines and state-of-the-art algorithms you will be able to use for comparison!\n",
        "1. **Evaluation**: This modules provides all the utilities and metrics that can help in evaluating a CL algorithm with respect to all the factors we believe to be important for a continually learning system.\n",
        "1. **Models**: In this module you'll be able to find several model architectures and pre-trained models that can be used for your continual learning experiment (similar to what has been done in [torchvision.models](https://pytorch.org/docs/stable/torchvision/index.html)).\n",
        "1. **Logging**: It includes advanced logging and plotting features, including native stdout, file and [Tensorboard](https://www.tensorflow.org/tensorboard) support (How cool it is to have a complete, interactive dashboard, tracking your experiment metrics in real-time with a single line of code?)\n",
        "\n",
        "In the graphic below, you can see how Avalanche sub-modules are available and organized as well:"
      ],
      "id": "Mqc2TP3aDeLa"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "name": "#%% raw\n"
        },
        "id": "oSaH1ihYDeLb"
      },
      "source": [
        "{% code title=\"Avalanche Main Modules and Sub-Modules\" %}\n",
        "```text\n",
        "Avalanche\n",
        "‚îú‚îÄ‚îÄ Benchmarks\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Classic\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Datasets\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Generators\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Scenarios\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Utils\n",
        "‚îú‚îÄ‚îÄ Evaluation\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Metrics\n",
        "|   ‚îî‚îÄ‚îÄ Utils\n",
        "‚îú‚îÄ‚îÄ Training\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Strategies\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Plugins\n",
        "|   ‚îî‚îÄ‚îÄ Utils\n",
        "‚îú‚îÄ‚îÄ Models\n",
        "‚îî‚îÄ‚îÄ Loggers\n",
        "\n",
        "```\n",
        "{% endcode %}"
      ],
      "id": "oSaH1ihYDeLb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RMf9Jt_iDeLc"
      },
      "source": [
        "We will learn more about each of them during this tutorial series, but keep in mind that the [Avalanche API documentation](https://avalanche-api.continualai.org/en/latest/) is your friend as well!\n",
        "\n",
        "All right, let's start with the benchmarks module right away üëá"
      ],
      "id": "RMf9Jt_iDeLc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pKzY7g6bDeLc"
      },
      "source": [
        "## üìö Benchmarks"
      ],
      "id": "pKzY7g6bDeLc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2vSAkXKKDeLc"
      },
      "source": [
        "The benchmark module offers three main features:\n",
        "1. **Datasets**: a comprehensive list of PyTorch Datasets ready to use (It includes all the Torchvision Datasets and more!).\n",
        "1. **Classic Benchmarks**: a set of classic Continual Learning Benchmarks ready to be used (there can be multiple benchmarks based on a single dataset).\n",
        "1. **Generators**: a set of functions you can use to generate your own benchmark starting from any PyTorch Dataset!"
      ],
      "id": "2vSAkXKKDeLc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "oiM7TqX8DeLd"
      },
      "source": [
        "### Datasets"
      ],
      "id": "oiM7TqX8DeLd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "iueuVmKPDeLd"
      },
      "source": [
        "Datasets can be imported in Avalanche as simply as:"
      ],
      "id": "iueuVmKPDeLd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-dzQ6XpLDeLd"
      },
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.datasets import MNIST, FashionMNIST, KMNIST, EMNIST, \\\n",
        "    QMNIST, FakeData, CocoCaptions, CocoDetection, LSUN, ImageNet, CIFAR10, \\\n",
        "    CIFAR100, STL10, SVHN, PhotoTour, SBU, Flickr8k, Flickr30k, VOCDetection, \\\n",
        "    VOCSegmentation, Cityscapes, SBDataset, USPS, HMDB51, UCF101, CelebA, \\\n",
        "    CORe50Dataset, TinyImagenet, CUB200, OpenLORIS, MiniImageNetDataset, \\\n",
        "    Stream51, CLEARDataset"
      ],
      "id": "-dzQ6XpLDeLd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lAxBI5kLDeLd"
      },
      "source": [
        "Of course, you can use them as you would use any *PyTorch Dataset*."
      ],
      "id": "lAxBI5kLDeLd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_1HJUxqsDeLe"
      },
      "source": [
        "### Benchmarks Basics"
      ],
      "id": "_1HJUxqsDeLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "s49_msZODeLe"
      },
      "source": [
        "The *Avalanche* benchmarks (instances of the Scenario class), contains several attributes that describe the benchmark. However, the most important ones are the `train` and `test` streams.\n",
        "\n",
        "In *Avalanche* we often suppose to have access to these **two parallel stream** of data (even though some benchmarks may not provide such feature, but contain just a unique test set).\n",
        "\n",
        "Each of these `streams` are iterable, indexable and sliceable objects that are composed of **experiences**. Experiences are batch of data (or \"tasks\") that can be provided with or without a specific task label."
      ],
      "id": "s49_msZODeLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OXCujZiaDeLe"
      },
      "source": [
        "### Classic Benchmarks"
      ],
      "id": "OXCujZiaDeLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "M9goqwqwDeLe"
      },
      "source": [
        "*Avalanche* maintains a set of commonly used benchmarks built on top of one or multiple datasets."
      ],
      "id": "M9goqwqwDeLe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Tkv9uh92DeLe"
      },
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.classic import CORe50, SplitTinyImageNet, SplitCIFAR10, \\\n",
        "    SplitCIFAR100, SplitCIFAR110, SplitMNIST, RotatedMNIST, PermutedMNIST, SplitCUB200\n",
        "\n",
        "# creating the benchmark (scenario object)\n",
        "perm_mnist = PermutedMNIST(\n",
        "    n_experiences=3,\n",
        "    seed=1234,\n",
        ")\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = perm_mnist.train_stream\n",
        "test_stream = perm_mnist.test_stream\n",
        "\n",
        "# iterating over the train stream\n",
        "for experience in train_stream:\n",
        "    print(\"Start of task \", experience.task_label)\n",
        "    print('Classes in this task:', experience.classes_in_this_experience)\n",
        "\n",
        "    # The current Pytorch training set can be easily recovered through the \n",
        "    # experience\n",
        "    current_training_set = experience.dataset\n",
        "    # ...as well as the task_label\n",
        "    print('Task {}'.format(experience.task_label))\n",
        "    print('This task contains', len(current_training_set), 'training examples')\n",
        "\n",
        "    # we can recover the corresponding test experience in the test stream\n",
        "    current_test_set = test_stream[experience.current_experience].dataset\n",
        "    print('This task contains', len(current_test_set), 'test examples')"
      ],
      "id": "Tkv9uh92DeLe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JmrvpYBbDeLf"
      },
      "source": [
        "### Benchmarks Generators"
      ],
      "id": "JmrvpYBbDeLf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rXcKooC6DeLf"
      },
      "source": [
        "What if we want to create a new benchmark that is not present in the *\"Classic\"* ones? Well, in that case Avalanche offers a number of utilities that you can use to create your own benchmark with maximum flexibility: the **benchmark generators**!\n",
        "\n",
        "The *specific* scenario generators are useful when starting from one or multiple PyTorch datasets and you want to create a **\"New Instances\"** or **\"New Classes\"** benchmark: i.e. it supports the easy and flexible creation of a *Domain-Incremental*, *Class-Incremental* or *Task-Incremental* scenarios among others."
      ],
      "id": "rXcKooC6DeLf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wj1YyxOrDeLf"
      },
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "mnist_train = MNIST('.', train=True, download=True)\n",
        "mnist_test = MNIST('.', train=False)\n",
        "\n",
        "benchmark = ni_benchmark(\n",
        "    mnist_train, mnist_test, n_experiences=10, shuffle=True, seed=1234,\n",
        "    balance_experiences=True\n",
        ")\n",
        "benchmark = nc_benchmark(\n",
        "    mnist_train, mnist_test, n_experiences=10, shuffle=True, seed=1234,\n",
        "    task_labels=False\n",
        ")"
      ],
      "id": "wj1YyxOrDeLf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LPPXIm0XDeLf"
      },
      "source": [
        "Finally, if your ideal benchmark does not fit well in the aforementioned *Domain-Incremental*, *Class-Incremental* or *Task-Incremental* scenarios, you can always use our **generic generators**:\n",
        "* **filelist_benchmark**\n",
        "* **paths_benchmark**\n",
        "* **dataset_benchmark**\n",
        "* **tensors_benchmark**"
      ],
      "id": "LPPXIm0XDeLf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_vGUINcFDeLf"
      },
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
        "                                            tensors_benchmark, paths_benchmark"
      ],
      "id": "_vGUINcFDeLf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Y6KOekL0DeLf"
      },
      "source": [
        "You can read more about how to use them the full *Benchmarks* module tutorial!"
      ],
      "id": "Y6KOekL0DeLf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5S8E63P9DeLg"
      },
      "source": [
        "## üí™ Training"
      ],
      "id": "5S8E63P9DeLg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8H8ugpecDeLg"
      },
      "source": [
        "The `training` module in *Avalanche* is build on modularity and it has two main goals:\n",
        "1. provide a set of standard *continual learning baselines* that can be easily run for comparison;\n",
        "1. provide the necessary utilities to **implement and run your own strategy** in the most efficient and simple way possible thanks to the building blocks we already prepared for you."
      ],
      "id": "8H8ugpecDeLg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Ub1RARwlDeLg"
      },
      "source": [
        "### Strategies"
      ],
      "id": "Ub1RARwlDeLg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gkai_675DeLg"
      },
      "source": [
        "If you want to compare your strategy with other classic continual learning algorithms or baselines, in *Avalanche* this is as simple as creating an object:"
      ],
      "id": "gkai_675DeLg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HI213cUsDeLg"
      },
      "outputs": [],
      "source": [
        "from avalanche.models import SimpleMLP\n",
        "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
        "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "model = SimpleMLP(num_classes=10)\n",
        "cl_strategy = Naive(\n",
        "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
        "    CrossEntropyLoss(), train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
        ")"
      ],
      "id": "HI213cUsDeLg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TRtnPP-SDeLg"
      },
      "source": [
        "### Create your own Strategy"
      ],
      "id": "TRtnPP-SDeLg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "EuWR_uB7DeLg"
      },
      "source": [
        "The simplest way to build your own strategy is to create a python class that implements the main `train` and `eval` methods.\n",
        "\n",
        "Let's define our Continual Learning algorithm **\"MyStrategy\"** as a simple python class:\n"
      ],
      "id": "EuWR_uB7DeLg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "prB3MyfODeLh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MyStrategy():\n",
        "    \"\"\"My Basic Strategy\"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def train(self, experience):\n",
        "        # here you can implement your own training loop for each experience (i.e. \n",
        "        # batch or task).\n",
        "\n",
        "        train_dataset = experience.dataset\n",
        "        t = experience.task_label\n",
        "        train_data_loader = DataLoader(\n",
        "            train_dataset, num_workers=4, batch_size=128\n",
        "        )\n",
        "\n",
        "        for epoch in range(1):\n",
        "            for mb in train_data_loader:\n",
        "                # you magin here...\n",
        "                pass\n",
        "\n",
        "    def eval(self, experience):\n",
        "        # here you can implement your own eval loop for each experience (i.e. \n",
        "        # batch or task).\n",
        "\n",
        "        eval_dataset = experience.dataset\n",
        "        t = experience.task_label\n",
        "        eval_data_loader = DataLoader(\n",
        "            eval_dataset, num_workers=4, batch_size=128\n",
        "        )\n",
        "\n",
        "        # eval here"
      ],
      "id": "prB3MyfODeLh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SLCVUx8nDeLh"
      },
      "source": [
        "Then, we can use our strategy as we would do for the pre-implemented ones:"
      ],
      "id": "SLCVUx8nDeLh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a4vvhxDqDeLh"
      },
      "outputs": [],
      "source": [
        "from avalanche.models import SimpleMLP\n",
        "from avalanche.benchmarks import SplitMNIST\n",
        "\n",
        "# Benchmark creation\n",
        "benchmark = SplitMNIST(n_experiences=5)\n",
        "\n",
        "# Model Creation\n",
        "model = SimpleMLP(num_classes=benchmark.n_classes)\n",
        "\n",
        "# Create the Strategy Instance (MyStrategy)\n",
        "cl_strategy = MyStrategy(\n",
        "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
        "    CrossEntropyLoss())\n",
        "\n",
        "# Training Loop\n",
        "print('Starting experiment...')\n",
        "\n",
        "for exp_id, experience in enumerate(benchmark.train_stream):\n",
        "    print(\"Start of experience \", experience.current_experience)\n",
        "\n",
        "    cl_strategy.train(experience)\n",
        "    print('Training completed')\n",
        "\n",
        "    print('Computing accuracy on the current test set')\n",
        "    cl_strategy.eval(benchmark.test_stream[exp_id])"
      ],
      "id": "a4vvhxDqDeLh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "X31vgs5JDeLh"
      },
      "source": [
        "While this is the easiest possible way to add your own strategy, *Avalanche* supports more sophisticated modalities (based on *callbacks*) that lets you write **more neat, modular and reusable code**, inheriting functionality from a parent classes and using **pre-implemented plugins**.\n",
        "\n",
        "Check out more details about what Avalanche can offer in this module following the \"*Training*\" chapter of the **\"From Zero to Hero\"** tutorial!"
      ],
      "id": "X31vgs5JDeLh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e2F5jowDDeLh"
      },
      "source": [
        "## üìà Evaluation"
      ],
      "id": "e2F5jowDDeLh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KH2ifI3fDeLh"
      },
      "source": [
        "The `evaluation` module is quite straightforward: it offers all the basic functionalities to evaluate and keep track of a continual learning experiment.\n",
        "\n",
        "This is mostly done through the **Metrics** and the **Loggers**. The **Metrics** provide a set of classes which implements the main continual learning metrics like Accuracy, Forgetting, Memory Usage, Running Times, etc.  \n",
        "Metrics should be created via the utility functions (e.g. `accuracy_metrics`, `timing_metrics` and others) specifying in the arguments when those metrics should be computed (after each minibatch, epoch, experience etc...).  \n",
        "The **Loggers** specify a way to report the metrics (e.g. with Tensorboard, on console or others). Loggers are created by instantiating the respective class.\n",
        "\n",
        "Metrics and loggers interact via the **Evaluation Plugin**: this is the main object responsible of tracking the experiment progress. Metrics and loggers are directly passed to the `EvaluationPlugin` instance. You will see the output of the loggers automatically during training and evaluation! Let's see how to put this together in few lines of code:"
      ],
      "id": "KH2ifI3fDeLh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PeOxYB8KDeLh"
      },
      "outputs": [],
      "source": [
        "# utility functions to create plugin metrics\n",
        "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, forgetting_metrics\n",
        "from avalanche.logging import InteractiveLogger, TensorboardLogger\n",
        "from avalanche.training.plugins import EvaluationPlugin\n",
        "\n",
        "eval_plugin = EvaluationPlugin(\n",
        "    # accuracy after each training epoch\n",
        "    # and after each evaluation experience\n",
        "    accuracy_metrics(epoch=True, experience=True),\n",
        "    # loss after each training minibatch and each\n",
        "    # evaluation stream\n",
        "    loss_metrics(minibatch=True, stream=True),\n",
        "    # catastrophic forgetting after each evaluation\n",
        "    # experience\n",
        "    forgetting_metrics(experience=True, stream=True), \n",
        "    # add as many metrics as you like\n",
        "    loggers=[InteractiveLogger(), TensorboardLogger()])\n",
        "\n",
        "# pass the evaluation plugin instance to the strategy\n",
        "# strategy = EWC(..., evaluator=eval_plugin)\n",
        "\n",
        "# THAT'S IT!!"
      ],
      "id": "PeOxYB8KDeLh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ivT57FyxDeLi"
      },
      "source": [
        "For more details about the evaluation module (how to write new metrics/loggers, a deeper tutorial on metrics) check out the extended guide in the *\"Evaluation\"* chapter of the **\"From Zero to Hero\"** *Avalanche* tutorial!"
      ],
      "id": "ivT57FyxDeLi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "oL3Cu6m8DeLi"
      },
      "source": [
        "## üîó Putting all Together"
      ],
      "id": "oL3Cu6m8DeLi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nxHAHf3CDeLi"
      },
      "source": [
        "You've learned how to install *Avalanche*, how to create benchmarks that can suit your needs, how you can create your own continual learning algorithm and how you can evaluate its performance.\n",
        "\n",
        "Here we show how you can use all these modules together to **design your experiments** as quantitative supporting evidence for your research project or paper."
      ],
      "id": "nxHAHf3CDeLi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lb_jCAAfDeLi"
      },
      "outputs": [],
      "source": [
        "from avalanche.benchmarks.classic import SplitMNIST\n",
        "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,\\\n",
        "    loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,\\\n",
        "    disk_usage_metrics, gpu_usage_metrics\n",
        "from avalanche.models import SimpleMLP\n",
        "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
        "from avalanche.training.plugins import EvaluationPlugin\n",
        "from avalanche.training import Naive\n",
        "\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "benchmark = SplitMNIST(n_experiences=5)\n",
        "\n",
        "# MODEL CREATION\n",
        "model = SimpleMLP(num_classes=benchmark.n_classes)\n",
        "\n",
        "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
        "# The evaluation plugin manages the metrics computation.\n",
        "# It takes as argument a list of metrics, collectes their results and returns \n",
        "# them to the strategy it is attached to.\n",
        "\n",
        "# log to Tensorboard\n",
        "tb_logger = TensorboardLogger()\n",
        "\n",
        "# log to text file\n",
        "text_logger = TextLogger(open('log.txt', 'a'))\n",
        "\n",
        "# print to stdout\n",
        "interactive_logger = InteractiveLogger()\n",
        "\n",
        "eval_plugin = EvaluationPlugin(\n",
        "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
        "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
        "    timing_metrics(epoch=True),\n",
        "    cpu_usage_metrics(experience=True),\n",
        "    forgetting_metrics(experience=True, stream=True),\n",
        "    StreamConfusionMatrix(num_classes=benchmark.n_classes, save_image=False),\n",
        "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
        "    loggers=[interactive_logger, text_logger, tb_logger]\n",
        ")\n",
        "\n",
        "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
        "cl_strategy = Naive(\n",
        "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
        "    CrossEntropyLoss(), train_mb_size=500, train_epochs=1, eval_mb_size=100,\n",
        "    evaluator=eval_plugin)\n",
        "\n",
        "# TRAINING LOOP\n",
        "print('Starting experiment...')\n",
        "results = []\n",
        "for experience in benchmark.train_stream:\n",
        "    print(\"Start of experience: \", experience.current_experience)\n",
        "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
        "\n",
        "    # train returns a dictionary which contains all the metric values\n",
        "    res = cl_strategy.train(experience, num_workers=4)\n",
        "    print('Training completed')\n",
        "\n",
        "    print('Computing accuracy on the whole test set')\n",
        "    # eval also returns a dictionary which contains all the metric values\n",
        "    results.append(cl_strategy.eval(benchmark.test_stream, num_workers=4))"
      ],
      "id": "lb_jCAAfDeLi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Tqm_pOh1DeLi"
      },
      "source": [
        "## ü§ù Run it on Google Colab\n",
        "\n",
        "You can run _this chapter_ and play with it on Google Colaboratory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContinualAI/avalanche/blob/master/notebooks/getting-started/learn-avalanche-in-5-minutes.ipynb)"
      ],
      "id": "Tqm_pOh1DeLi"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}