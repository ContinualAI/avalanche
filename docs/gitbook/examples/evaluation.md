---
description: Protocols and Metrics Code Examples
---

# Evaluation

_Avalanche_ offers significant support for _defining your own eveluation protocol_ (classic or custom metrics, when and on what to test). You can find **examples** related to the benchmarks here:&#x20;

* [Eval Plugin](../../../examples/eval\_plugin.py): _this is a simple example on how to use the Evaluation Plugin (the evaluation controller object)_
* [Standalone Metrics](../../../examples/standalone\_metric.py): _how to use metrics as standalone objects._&#x20;
* [Confusion Matrix](../../../examples/confusion\_matrix.py): _this example shows how to produce confusion matrix during training and evaluation._
* [Dataset Inspection](../../../examples/dataset\_inspection.py)_: this is a simple example on how to use the Dataset inspection plugins._
* [Mean Score](../../../examples/mean\_scores.py): _example usage of the mean\_score helper to show the scores of the true class, averaged by new and old classes._
* __[_Task Metrics_](../../../examples/task\_metrics.py)_: this is a simple example on how to use the Evaluation Plugin with metrics returning values for different tasks._
