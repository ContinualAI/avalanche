---
description: Protocols and Metrics Code Examples
---

# Evaluation

_Avalanche_ offers significant support for _defining your own eveluation protocol_ (classic or custom metrics, when and on what to test). You can find **examples** related to the benchmarks here:&#x20;

* [Eval Plugin](../../../examples/eval_plugin.py): _this is a simple example on how to use the Evaluation Plugin (the evaluation controller object)_
* [Standalone Metrics](../../../examples/standalone_metric.py): _how to use metrics as standalone objects._&#x20;
* [Confusion Matrix](../../../examples/confusion_matrix.py): _this example shows how to produce confusion matrix during training and evaluation._
* [Dataset Inspection](../../../examples/dataset_inspection.py)_: this is a simple example on how to use the Dataset inspection plugins._
* [Mean Score](../../../examples/mean_scores.py): _example usage of the mean\_score helper to show the scores of the true class, averaged by new and old classes._
* [_Task Metrics_](../../../examples/task_metrics.py)_: this is a simple example on how to use the Evaluation Plugin with metrics returning values for different tasks._
