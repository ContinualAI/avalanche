---
description: Benchmarks and DatasetCode Examples
---

# Benchmarks

_Avalanche_ offers significant support for _defining your own benchmarks_ (instantiation of one scenario with one or multiple datasets) or using "**classic" benchmarks** already consolidate in the literature.

You can find **examples** related to the benchmarks here:&#x20;

* [Classic MNIST benchmarks](../../../examples/all\_mnist.py): _in this simple example we show all the different ways you can use MNIST with Avalanche._
* __[SplitCifar100 benchmark](../../../examples/lamaml\_cifar100.py): _in this example a CIFAR100 is used with its canonical split in 10 experiences, 10 classes each._
* __[CLEAR benchmark](../../../examples/clear.py): _training and evaluating on CLEAR benchmark (RGB images)_
* [CLEAR Linear benchmark](../../../examples/clear\_linear.py): _Training and evaluating on CLEAR benchmark (with pre-trained features)_
* [Detection Benchmark](../../../examples/detection\_examples\_utils.py)_: about the utils you can use create a detection benchmark._
* [Endless CL Simulator](../../../examples/endless\_cl\_sim.py)_: this example makes use of the Endless-Continual-Learning-Simulator's derived dataset scenario._
* [Simple CTRL benchmark](../../../examples/simple\_ctrl.py): _In this example we show a simple way to use the ctrl benchmark_.&#x20;
* [Task-Incremental Learning](../../../examples/task\_incremental.py): _this example trains on Split CIFAR10 with Naive strategy. In this example each experience has a different task label._
* [HuggingFace integration](../../../examples/nlp.py)_: how to use HuggingFace models and datasets within Avalanche for Natural Language Processing._
